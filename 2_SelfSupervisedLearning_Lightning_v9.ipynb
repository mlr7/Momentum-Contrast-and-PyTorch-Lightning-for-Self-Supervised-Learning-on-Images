{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lightning and Lightly for Contrastive Self-Supervised Learning with MoCo\n",
    "\n",
    "PyTorch Lightning is a high-level framework built on top of PyTorch. Lightning is designed to abstract away the boilerplate code required for deep learning projects, allowing Lightning users to focus more on the research and less on the underlying engineering complexity.\n",
    "\n",
    "Self-supervised learning (SSL) has emerged as a powerful strategy for scaling up the amount of data that can be leveraged for training machine learning models. In SSL labels are generated from unlabeled datasets by ingeniously leverages properties of the data itself. This facilitates the learning of rich representations without the need for manually annotating datasets. \n",
    "\n",
    "Contrastive learning is an SSL approach that teaches a model to distinguish between similar (positive) and dissimilar (negative) pairs of data points. It relies on creating embeddings in a way that similar or \"positive\" pairs are brought closer together, while dissimilar or \"negative\" pairs are pushed apart in the embedding space. This is achieved through a contrastive loss function, such as Noise Contrastive Estimation (NCE) or Triplet Loss.\n",
    "\n",
    "Momentum Contrast (MoCo) is a specific instantiation of contrastive learning designed to address the challenge of having a large and consistent set of negative examples. MoCo achieves this by maintaining a dynamic dictionary of data samples, using a momentum-updated encoder. This strategy allows for a larger and more consistent set of negatives over batches, improving the quality of the learned representations. MoCo can be seen as enhancing the scalability and effectiveness of contrastive learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from lightly.transforms import MoCoV2Transform, utils\n",
    "from lightly.data import LightlyDataset\n",
    "\n",
    "# MoCo SSL Model Definition\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# MoCo\n",
    "\n",
    "from lightly.models import ResNetGenerator\n",
    "from lightly.models.modules.heads import MoCoProjectionHead\n",
    "from lightly.loss import NTXentLoss\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from lightly.models.utils import (\n",
    "    batch_shuffle,\n",
    "    batch_unshuffle,\n",
    "    deactivate_requires_grad,\n",
    "    update_momentum,\n",
    ")\n",
    "\n",
    "import copy\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from pytorch_lightning.loggers import CSVLogger # capture lightning training data\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading parameters\n",
    "num_workers = 8\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoCo parameters\n",
    "memory_bank_size = 4096\n",
    "seed = 1\n",
    "max_epochs = 100  #2 #5 #100 # 5 to see execution flow, 100 for actual results, to start with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Dataloaders will load and preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Paths to png images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = \"../data_cifar10/train/\"\n",
    "path_to_test = \"../data_cifar10/test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataloader for MoCo Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable blur because we're working with tiny images\n",
    "transform = MoCoV2Transform(\n",
    "    input_size=32,\n",
    "    gaussian_blur=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the moco augmentations for training moco\n",
    "dataset_train_moco = LightlyDataset(input_dir=path_to_train, \n",
    "                                    transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train_moco = torch.utils.data.DataLoader(\n",
    "    dataset_train_moco,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataloader for Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentations typically used to train on cifar-10\n",
    "train_classifier_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.RandomCrop(32, padding=4),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=utils.IMAGENET_NORMALIZE[\"mean\"],\n",
    "            std=utils.IMAGENET_NORMALIZE[\"std\"],\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note on augmentations\n",
    "\n",
    "We will be training a linear classifier using the already-prepared MoCo model, incorporating the same test augmentations. The augmentations from MoCo are potent, often leading to a decrease in accuracy for models not designed for contrastive learning. The training of our linear layer will utilize cross entropy loss, guided by the dataset's labels. As a result, we'll opt for milder augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_classifier = LightlyDataset(\n",
    "    input_dir=path_to_train, \n",
    "    transform=train_classifier_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train_classifier = torch.utils.data.DataLoader(\n",
    "    dataset_train_classifier,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataloader for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No additional augmentations for the test set\n",
    "test_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize((32, 32)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=utils.IMAGENET_NORMALIZE[\"mean\"],\n",
    "            std=utils.IMAGENET_NORMALIZE[\"std\"],\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = LightlyDataset(input_dir=path_to_test, \n",
    "                              transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MoCo Model Definition with PyTorch-Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MocoModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\", 1, num_splits=8)\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1],\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "\n",
    "        # create a moco model based on ResNet\n",
    "        self.projection_head = MoCoProjectionHead(512, 512, 128)\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "        deactivate_requires_grad(self.backbone_momentum)\n",
    "        deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        # create our loss with the optional memory bank\n",
    "        self.criterion = NTXentLoss(\n",
    "            temperature=0.1, memory_bank_size=(memory_bank_size, 128)\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x_q, x_k), _, _ = batch\n",
    "\n",
    "        # update momentum\n",
    "        update_momentum(self.backbone, self.backbone_momentum, 0.99)\n",
    "        update_momentum(self.projection_head, self.projection_head_momentum, 0.99)\n",
    "\n",
    "        # get queries\n",
    "        q = self.backbone(x_q).flatten(start_dim=1)\n",
    "        q = self.projection_head(q)\n",
    "\n",
    "        # get keys\n",
    "        k, shuffle = batch_shuffle(x_k)\n",
    "        k = self.backbone_momentum(k).flatten(start_dim=1)\n",
    "        k = self.projection_head_momentum(k)\n",
    "        k = batch_unshuffle(k, shuffle)\n",
    "\n",
    "        loss = self.criterion(q, k)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.custom_histogram_weights()\n",
    "\n",
    "    # We provide a helper method to log weights in tensorboard\n",
    "    # which is useful for debugging.\n",
    "    def custom_histogram_weights(self):\n",
    "        for name, params in self.named_parameters():\n",
    "            self.logger.experiment.add_histogram(name, params, self.current_epoch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=6e-2,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the MoCo model\n",
    "model = MocoModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MoCo model using the lightning trainer\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"cpu\") \n",
    "\n",
    "# trainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\n",
    "# MisconfigurationException: No supported gpu backend found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, dataloader_train_moco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Linear Classifier Using Extracted MoCo Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        # use the pretrained ResNet backbone\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # freeze the backbone\n",
    "        deactivate_requires_grad(backbone)\n",
    "\n",
    "        # create a linear layer for our downstream classification model\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_hat = self.backbone(x).flatten(start_dim=1)\n",
    "        y_hat = self.fc(y_hat)\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"train_loss_fc\", loss)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.custom_histogram_weights()\n",
    "\n",
    "    # We provide a helper method to log weights in tensorboard\n",
    "    # which is useful for debugging.\n",
    "    def custom_histogram_weights(self):\n",
    "        for name, params in self.named_parameters():\n",
    "            self.logger.experiment.add_histogram(name, params, self.current_epoch)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat = torch.nn.functional.softmax(y_hat, dim=1)\n",
    "\n",
    "        # calculate number of correct predictions\n",
    "        _, predicted = torch.max(y_hat, 1)\n",
    "        num = predicted.shape[0]\n",
    "        correct = (predicted == y).float().sum()\n",
    "        self.validation_step_outputs.append((num, correct))\n",
    "        return num, correct\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # calculate and log top1 accuracy\n",
    "        if self.validation_step_outputs:\n",
    "            total_num = 0\n",
    "            total_correct = 0\n",
    "            for num, correct in self.validation_step_outputs:\n",
    "                total_num += num\n",
    "                total_correct += correct\n",
    "            acc = total_correct / total_num\n",
    "            self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "            self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(self.fc.parameters(), lr=30.0)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier(model.backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\n",
    "trained = []\n",
    "#trainer = pl.Trainer(max_epochs=10, devices=1, accelerator=\"gpu\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, \n",
    "                     devices=1, \n",
    "                     accelerator=\"gpu\", # \"auto\"\n",
    "                     logger=CSVLogger(save_dir=\"logs2/\"),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(classifier, \n",
    "            dataloader_train_classifier, \n",
    "            dataloader_test)\n",
    "\n",
    "#output_train1 = trainer.fit(classifier, \n",
    "#                            dataloader_train_classifier, \n",
    "#                            dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(output_train1)\n",
    "trainer.logger.log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\n",
    "\n",
    "# see PyToch lightning documentation for how to extract csv of training output\n",
    "# https://lightning.ai/docs/pytorch/stable/common/trainer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightly shows a lot of potential for simplifying SSL workflows, and the integration of Lightly with PyTorch-Lightning is a great idea. The Lightly team should work on additional documentation for extracting SSL training results and documentation their impact on downstream tasks such as image classification. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
